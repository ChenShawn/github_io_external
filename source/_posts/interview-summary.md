---
title: 论面试官虐我的一百种方式
date: 2018-12-30 19:02:03
tags: Personal
mathjax: true
---

## ByteDance抖音火山组
<!-- more -->
### 一面

- 自我介绍
- 大致地聊一下简历，总结自己的研究方向，简述自己的文章工作，简述项目与之前的实习经历
- Adam优化的原理与理解：momentum与RMSProp的结合，一阶矩滑动平均稳定迭代方向，二阶矩作为cache控制迭代步长，在带有噪声的loss surface上效果较好
- 手撕代码第一题：开根号，二分查找与牛顿法两种解法，注意迭代停止条件的设置
- 介绍Attention mechanism：soft attention、hard attention，self-attention，其中soft attention问题可拓展至seq2seq和LSTM等，self-attention拓展至CNN-based attention及其应用
- 介绍自己研究方向的几个主流方法及其优缺点

### 二面

- 手撕代码第二题：Resovior sampling
- 手撕代码第三题：二维数组找最长递增路径，DFS题
- 深度学习框架如TensorFlow如何实现并行：两个维度，GPU并行计算与计算图级别的并行，前者问题可以延伸至CUDA编程中的一些基础知识，后者可以延伸至PS架构
- 解释BP实现过程
- 问简历细节，dCRF和LevelSet原理与实现
- 重复问了Attention mechanism

### 三面

- 手撕代码第四题：finding the median in two sorted array，至少得写出log(M+N)级别的解法
- 不看简历，叙述以前的项目和比赛，如果方向不是强相关的话会直接打断
- Random forest和GBDT，节点如何分裂，用什么metric来选特征，要求写出公式；进一步问题还可以延伸至feature importance怎么算，xgboost原理与实现等
- 介绍Attention mechanism ~~(我简历上都没写为啥这么喜欢问attention啊摔)~~
- 推荐搜索相关：优化搜索引擎时，有哪些指标可以作为评判用户满意程度与优化用户体验的依据

### AML机器学习平台组

*由于求职意向不符总共没聊几句，之后官网状态更新，头条以后再也不见*

<div>
    <img src="http://img.itlun.cn/uploads/allimg/160416/1-160416140051.jpg" width="200px">
</div>

- 详细问了之前实习中的内容，如何旁路log，如何提高性能
- C++多态底层实现
- 一个线段随机分成三份，求三条子线段正好可以构成一个三角形的概率
- 旋转数组求最小值下标

## Baidu Feed流架构组

### 一面

- 自我介绍
- 重载与虚函数的底层实现机制
- 内存有哪几种，堆空间与栈空间的区别
- 手撕代码第一题：ThreeSum，要求bug free
- 哈希表如何解决冲突，拉链式与线性查找，两种方案的优缺点，缺点如何解决

### 二面

- 聊简历，聊论文的工作，为什么可以取得比较好的效果
- 有哪些可能会涉及到内存泄漏的场景，如何避免内存泄漏
- 手撕代码第二题：`char* strdump(const char* s)`，复制字符串，要求bug free
- 智能指针的原理与实现，开放式问题：工程中是否应该用智能指针
- 操作系统相关：C++11对多线程的支持，口头叙述生产者消费者读写buffer过程，虚拟内存与物理内存，Linux的top指令
- 简单问了下new operator原理，boost中内存池的原理
- 手写一个含有动态内存分配成员的类的各种函数签名，考点在于RAII、拷贝构造、右值拷贝与operator=
- 网络相关：TCP三次握手过程，浏览器浏览网页过程中涉及到的网络协议，select、poll和epoll

## 蚂蚁金服

### 一面

- 介绍论文的工作
- linear regression，推进式地问了四个问题
  - $Ax=b$ 欠定方程组求解
  - 若$A\in{R^{m\times{n}}}$，求复杂度
  - 如何降低求逆运算的复杂度 => 引出SVD或QR分解
  - 在此问题中SVD为什么能够降低求逆复杂度 => 考察SVD细节
- 算法也是渐进式地问了几个问题
  - 一个数组$[1, 2, 3, ..., N]$随机打乱并从中去掉一个元素，如何找到这个元素，时间$O(N)$，空间$O(1)$
  - 如果从中去掉两个元素如何求解，复杂度同上 => 分治
  - 用数学方法求解出缺少的两个元素的公式

### 二面

- 介绍论文的工作，会问到每种方法的复杂度和缺点，改进方法，在其他领域是否可以有应用
- 详细地问项目细节
- RF、GBDT的区别，bagging和boosting的区别，xgboost如何并行，和RF比哪个快
- 有没有用过分布式框架 => gRPC，brpc => 开始问之前的实习，RPC如何工作，brpc有什么特性
- 乱序数组，找中位数 => Partition variant

### 三面

- 介绍论文，你的论文对于工业界来讲可以在哪些方面得到应用
- xgboost和random forest的优缺点
- 如何处理离散特征（类别型特征）
- 特征工程中，若数据本身的维度为$N$，那么对特征两两进行交叉组合可以得到$N^{2}级别的特征量，如何快速筛选组合特征
- 如果进蚂蚁金服的话，你希望从事哪方面的工作？

### 四面

*和之前的面试相比，很多问题都感觉让人摸不着头脑，不知道面试官到底想问什么，感觉自己在答一些非技术问题的时候也踩了一些雷，不知道是交叉面还是HR面还是老板面*

- 自我介绍，讲论文，常规操作
- 介绍自己的三个优点与三个缺点 => ~~喵喵喵~~ => 面试官：看来以前没有自己想过这个问题哈
- 你希望自己从事偏科研还是偏工程的工作，是否有读博打算
- 你是如何学习机器学习的，介绍几个机器学习深度学习领域的代表人物
- 你的模型处于低偏差高方差的状态，应该采用什么**算法**来解决（不懂这个问题的点在哪里，我思考的逻辑如下
  - 决策树类的模型，post-pruning一类或许可以勉强称之为“算法”
  - 神经网络这类capacity极大的模型，adversarial training应该也算是算法
  - 其他诸如dropout、数据增强、加regularizer等等，都不能算得上是“算法”，且不指定“模型”是什么的前提下，完全没有办法回答用什么**算法**解决过拟合
- 介绍什么是过拟合，如何解决
- 8个球长得一模一样，其中一个比其他球更重，给一个天平，最少多少次可以找到那个最重的球（每次问智商题都会暴露出智商的不足，正确答案是两次
- 一个搜索引擎，新算法上线做AB test，新算法线下测试的结果比旧算法好，但由于工程错误实际表现不如旧的算法，你认为可能是由于什么原因，如何理解这一类问题（~~全然意味わからない~~

## Tencent TEG

### 一面

- 介绍简历上的工作，详细讲了一作的论文，非一作的直接跳过
- 介绍简历上的项目，说到CRF as RNN模型的时候，面试官问你们的训练数据量远少于语义分割，具体是如何训练以及对抗过拟合的，是否有用到pretrained model
  - 大部分做主流CV任务的网络参数量都太大了，不适合直接迁移
  - 将VGG换成了UNet => 追问，为什么换
  - 常规的数据增强 => 有哪几种
  - Adversarial training => 追问了具体实现方法 => 参考了[ICLR 2018 Sinha *et al*](https://arxiv.org/abs/1710.10571)的工作
- 针对王者荣耀任务设计强化学习算法框架（复盘时想起腾讯AILab在ICML2018上的论文上有一张王者荣耀比赛的图片，大概面试官是想听我说MCTS，可惜当时没有想到这一层，捂脸
  - 问面试官同时控制五个英雄会不会涉及到multi-agent的问题 => 先简化问题，认为只有一个英雄也可以
  - Hirachical RL，比如分开对线期和gank期子任务等等
  - 大型MDP，baseline模型可以用DDPG或者distributed PPO
  - 大量的人类操作数据，可以做imitation learning => 追问了具体该怎么做
    - 说到Inverse RL，讲了下DAgger
    - 介绍了GAIL以及AIRL的一系列工作
- 平时玩游戏吗 => 玩 => 面试官说那么我们部门做的东西你应该蛮感兴趣的

### 二面

- 怼项目，面试结束之后复盘发现，其实面试官问的有关项目的一连串问题，是希望我可以按照一篇学术论文的思路，将整个项目的历程组织起来的：
  - **Background & Motivation:** 项目中用到的方法既然是参考别人的paper做的，那么你如何理解这次项目与别人paper中遇到的问题的差别？
  - **Related works:** 传统上其他类似的视觉任务遇到这种问题是怎么做的 => 介绍了RNN as CRF，PSPNet，DeepLab
  - **Our approach:** 既然你的问题与语义分割不同，为了解决task-specific problems你们做了哪些改进 => 为什么这样设计网络结构 => dense CRF具体怎么实现的
  - **Conclusion & Future works:** 做完这个项目你觉得里面还有什么问题是待解决或未解决的
- 怼论文，之前有没有人做过类似的工作，与自己的方法比较，还有一些论文的琐碎细节问题
- 详细介绍深度学习网络结构的发展历程，从AlexNet到VGG再到ResNet再到DenseNet => 延伸问题：为什么DenseNet效果可以比ResNet更好
- 介绍深度学习优化方法的研究脉络与发展历程，从SGD到Momentum再到Adagrad和RMSProp，最后详细讲Adam
- 详细叙述SVM，如何解决线性不可分问题
- 两个数组取交集，讲算法，推复杂度（~~貌似推错了~~
- 详细问了有关RL的一系列问题，分value-based方法和policy-based方法
  - policy-based方法和value-based主要的区别在于哪里 => 从Bellman equation开始各种胡扯
  - value-based方法学习的目标是什么
  - 讲policy-based方法的研究脉络与发展历程 => 从policy gradient theorem讲到REINFORCE，从DDPG讲到A2C，从TRPO讲到PPO
    - 追问一：value function在TRPO中的作用是什么
    - 追问二：带value function的模型在优化时如何迭代
    - 追问三：value function的loss可不可以和policy的loss放到同一个框架下
    - 追问四：介绍PPO的两种objective function
- 聊人生，为什么做RL => 觉得有趣
- 重复问了一面的问题，平时玩游戏吗 => 玩 => 玩什么游戏 => Dota2（~~腾讯的面试者里面上一个这样回答的已经被拉黑了~~

### HR面

- 问我的具体研究方向 => RL强相关方向
- 由于我学校在上海，问我去深圳工作有没有问题
- 之前对腾讯有什么了解 => 看过AILab发的顶会文章，仰望大佬
- 有没有亲戚朋友之类的在深圳 => 没有 => 将来考虑留深圳吗 => 考虑
- 什么时候可以开始上班

*然后HR就直接口头通知说offer会在月底发到我手上，结束*

## 其他算法相关

- 一个长度为$N$的数组，$N$为偶数，将其平均分成两部分使得两部分和的乘积最大，背包题
- 给$2k+1$个硬币，问扔完之后正面比反面多的概率是多少
  - $\frac{1}{2^{2k+1}}\sum_{i=k+1}^{2k+1}C_{2k+1}^{i}$
  - 有公式$\sum_{i=0}^{N}C_{N}^{i}=2^{N}$，$C_{N}^{0}+C_{N}^{2}+...+C_{N}^{N}=2^{N-1}$，由于$C_{N}^{i}=C_{N}^{N-i}$，上式化简得答案1/2
- 给定二维平面上点集$\{p_{1},...,p_{n} | p_{i}=(x_{i},y_{i})\}$，定义$p_{i}$为最大当且仅当对于所有的$p_{j},\ j\neq{i}$，都满足条件$x_{i}>x_{j}\ \text{or}\ y_{i}>y_{j}$，求最大点点集，复杂度$O(N\log(N))$

## 其他机器学习相关

- HMM与CRF
- Overfitting解决，除了常规的加正则，增加训练数据，数据增强，减少模型参数以外，还可以加adversarial training，把模型看作是PGM加hidden variable
- 推荐系统题：给十亿个用户特征向量，对于每个用户找到与其最相似的十个用户
- xgboost相比GBDT和RF的优势
- 模型度量：AUC、ROC，recall
- batch norm的原理与理解
