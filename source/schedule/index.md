---
title: TODO list
date: 2019-03-11 23:33:21
type: "Schedule"
---

## 工作

两篇有关估计policy gradient系列方法方差的文章，都出现于ICLR-2019，可以深入研究下如何理解policy gradient中的方差

- [Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications](https://arxiv.org/abs/1806.05134)
- [Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines](https://arxiv.org/abs/1803.07246)

有时间再仔细研究下[soft actor-critic](https://arxiv.org/abs/1801.01290)中的理论部分，这篇文章的内容很硬核，也有很多后续工作

### *References*

- [知乎大佬的专栏](https://zhuanlan.zhihu.com/reinforcementlearning)，大佬follow前沿工作总是要比我快很多，看文章的眼光也很独到，值得学习

## 生活

- 预约报销
- 出行程单

## High Priority

<div class="high-priority">
    <img src="https://t3.ftcdn.net/jpg/01/12/94/58/240_F_112945864_DWYdvoginnPSab8pg8yhYd0QCJJe3EH7.jpg" style="opacity:0.3" width="100%">

    <div class="high-priority-inner">
        <ul>
            <li>Place holder 1</li>
            <li>Placeholder</li>
        </ul>
    </div>
</div>